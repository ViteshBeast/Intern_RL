# Import necessary libraries
import os
import cv2
import gym
from gym import spaces
import numpy as np
import random
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Dataset Loader
def load_dataset(dataset_dir, target_size=(256, 256)):
    images = []
    annotations = []
    for phase in ['train', 'val', 'test']:
        images_dir = os.path.join(dataset_dir, phase, 'images')
        annotations_dir = os.path.join(dataset_dir, phase, 'labels')
        
        for img_name in os.listdir(images_dir):
            img_path = os.path.join(images_dir, img_name)
            annotation_path = os.path.join(annotations_dir, img_name.replace('.jpg', '.txt'))
            
            if not os.path.exists(annotation_path):
                continue  # Skip if annotation file does not exist
            
            image = cv2.imread(img_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB
            image = cv2.resize(image, target_size)  # Resize image
            image = image.astype(np.float32) / 255.0  # Normalize and convert to float32
            
            with open(annotation_path, 'r') as f:
                annotation_lines = f.readlines()
                annotation = [list(map(float, line.strip().split())) for line in annotation_lines]
            
            images.append(image)
            annotations.append(annotation)
    
    return images, annotations

#Function to give reward for movement action
reward_movement_action=1

def get_reward_movement(iou, new_iou):
    if new_iou > iou:
        reward = reward_movement_action
    else:
        reward = - reward_movement_action
    return reward

# Object Detection Environment
class ObjectDetectionEnv(gym.Env):
    def __init__(self, images, annotations):
        super(ObjectDetectionEnv, self).__init__()
        self.images = images
        self.annotations = annotations
        self.current_index = 0
        
        # Define action and observation space
        self.action_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)  # x, y, width, height
        self.observation_space = spaces.Box(low=0, high=1, shape=images[0].shape, dtype=np.float32)
    #function to return to initial state
    def reset(self):
        self.current_index = np.random.randint(0, len(self.images))
        self.current_image = self.images[self.current_index]
        self.current_annotations = self.annotations[self.current_index]
        print(f"Annotations: {self.current_annotations}")  # Debug print
        return self.current_image
    
    
    
    
    #function to perform action in the environment.
    def step(self, action):
        if isinstance(action, np.ndarray):
            action = action.tolist()
        x, y, w, h = action
        total_reward = 0
        max_iou = 0
        
        #iou between box after action and ground-truth box
        for annotation in self.current_annotations:
            gt_class, gt_x, gt_y, gt_w, gt_h = annotation
            iou = self.calculate_iou([x, y, w, h], [gt_x, gt_y, gt_w, gt_h])
            max_iou = max(max_iou, iou)
            
        iou_threshold=0.5
        reward_terminal_action=3
        #positive terminal reward is added to total reward if iou is greater than threshold and vice-versa.
        if max_iou >= iou_threshold:
            reward = reward_terminal_action
        else:
            reward = -reward_terminal_action
        #to add movement reward to total reward after action
        for annotation in self.current_annotations:
            gt_class, gt_x, gt_y, gt_w, gt_h = annotation
            new_iou = self.calculate_iou([x, y, w, h], [gt_x, gt_y, gt_w, gt_h])
            reward += get_reward_movement(iou, new_iou)
        
        total_reward += reward
        
        # Assuming episode ends after each step for simplicity
        done = True
        info = {}
        
        return self.current_image, total_reward, done, info
    
    #function to calculate iou between predicted and ground truth box.
    def calculate_iou(self, box1, box2):
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        
        xi1 = max(x1 - w1/2, x2 - w2/2)
        yi1 = max(y1 - h1/2, y2 - h2/2)
        xi2 = min(x1 + w1/2, x2 + w2/2)
        yi2 = min(y1 + h1/2, y2 + h2/2)
        
        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
        box1_area = w1 * h1
        box2_area = w2 * h2
        
        iou = inter_area / (box1_area + box2_area - inter_area)
        return iou
    #function to plot bounding box on image
    def draw_bounding_box(self, image, box, color=(0, 255, 0), thickness=2):
        x, y, w, h = box
        height, width, _ = image.shape
        x1, y1 = int((x - w / 2) * width), int((y - h / 2) * height)
        x2, y2 = int((x + w / 2) * width), int((y + h / 2) * height)
        return cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)
    #function to render image with bounding boxes
    def render(self, mode='human'):
        image = self.current_image.copy()
        detected_box = self.action_space.sample()
        for annotation in self.current_annotations:
            gt_class, gt_x, gt_y, gt_w, gt_h = annotation
            gt_box = [gt_x, gt_y, gt_w, gt_h]
            image = self.draw_bounding_box(image, gt_box, color=(255, 0, 0))  # Red for ground truth
        image = self.draw_bounding_box(image, detected_box, color=(0, 255, 0))  # Green for detected
        plt.imshow(image)
        plt.show()

# DQN Model
class DQN(tf.keras.Model):
    def __init__(self, action_space):
        super(DQN, self).__init__()
        self.conv1 = layers.Conv2D(32, (8, 8), strides=4, activation='relu')
        self.conv2 = layers.Conv2D(64, (4, 4), strides=2, activation='relu')
        self.conv3 = layers.Conv2D(64, (3, 3), strides=1, activation='relu')
        self.flatten = layers.Flatten()
        self.fc1 = layers.Dense(512, activation='relu')
        self.fc2 = layers.Dense(action_space.shape[0], activation='sigmoid')  # x, y, w, h in range [0, 1]
    #to input the output of one layer to next layer 
    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.flatten(x)
        x = self.fc1(x)
        return self.fc2(x)

# Main script
dataset_dir = 'Train_Dataset'
images, annotations = load_dataset(dataset_dir, target_size=(256, 256))  # Resize images to 256x256

# Initialize environment
env = ObjectDetectionEnv(images, annotations)

# Initialize model
model = DQN(env.action_space)
optimizer = Adam(learning_rate=1e-4)

# Hyperparameters
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.1
epsilon_decay = 0.995
batch_size = 32
memory = []
#function to keep action in experience replay
def remember(state, action, reward, next_state, done):
    memory.append((state, action, reward, next_state, done))
#function to sample some actions from experience replay
def replay():
    if len(memory) < batch_size:
        return
    minibatch = random.sample(memory, batch_size)
    for state, action, reward, next_state, done in minibatch:
        target = reward
        if not done:
            target += gamma * np.amax(model(next_state[np.newaxis, ...])[0])
        target_f = model(state[np.newaxis, ...])
        target_f[0][action] = target
        model.train_on_batch(state[np.newaxis, ...], target_f)

def act(state):
    if np.random.rand() <= epsilon:
        return env.action_space.sample().tolist()  # Ensure action is a list
    act_values = model(state[np.newaxis, ...])
    return act_values[0].numpy().tolist()  # Ensure action is a list

# Training loop
for e in range(1000):  # Number of episodes
    state = env.reset()
    state = np.array(state)
    for time in range(500):  # Max steps per episode
        action = act(state)
        next_state, reward, done, _ = env.step(action)
        next_state = np.array(next_state)
        remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            print(f"Episode {e+1}/{1000} - Time: {time}, Reward: {reward}")
            env.render()  # Show image with bounding boxes
            break
        replay()
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay
